{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week I'm working on pytorch basis and the attention mechanism commonly used in LLMs: causal attention.\n",
    "\n",
    "Thank you, Sebastian Raschka, for the excellent examples and explanations in Build A Large Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Test if MPS is available on the machine\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pytorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([1, 3, 3, 7])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# tensors come in all shapes and sizes in deep learning.\n",
    "\n",
    "tensor_0d = torch.tensor(1)\n",
    "\n",
    "tensor_1d = torch.tensor([1, 3, 3, 7])\n",
    "\n",
    "tensor_2d = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4]])\n",
    "\n",
    "tensor_3d = torch.tensor([\n",
    "    [\n",
    "        [1, 2],\n",
    "        [3, 4]\n",
    "    ],\n",
    "    [\n",
    "        [5, 6],\n",
    "        [7, 8]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# print all the tensors\n",
    "print(tensor_0d)\n",
    "print(tensor_1d)\n",
    "print(tensor_2d)\n",
    "print(tensor_3d)\n",
    "\n",
    "# hmmm... the 3d is hard to visualise looking at this example. Now, I see it.\n",
    "# It's like 2 2d tensors stacked like plates on a dish rack lol.\n",
    "\n",
    "print(tensor_3d.shape)\n",
    "print(tensor_3d.dtype)\n",
    "\n",
    "# Ok. I can see torch defaults to Int64. It'll default to float32 if we provide a float.\n",
    "\n",
    "float_tensor_1d = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(float_tensor_1d.dtype)\n",
    "\n",
    "# Before we move on, most GPUs are optimised for float32. And generally speaking, float32\n",
    "# is more than enough precision for most deep learning tasks. However, you'll see \n",
    "# a lot of folks are GPU poor, so they use all sorts of tricks to get by with less precision (e.g., bfloat16, int8, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4]])\n",
      "False\n",
      "torch.Size([2, 2])\n",
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "torch.Size([2, 2]) torch.Size([2, 2]) torch.Size([2, 2])\n",
      "tensor([[ 5, 11],\n",
      "        [11, 25]])\n",
      "tensor(2)\n",
      "tensor([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Like linear algebra, the torch provides common operations for tensors.\n",
    "\n",
    "# 1) reshaping is important\n",
    "\n",
    "print(tensor_2d.reshape(1, 4)) # tensor([[1, 2, 3, 4]])\n",
    "\n",
    "# but view is the preferred method for reshaping tensors\n",
    "# because it requires the tensor to be contiguous in memory\n",
    "# and will fail if the tensor isn't. this sounds safer to me.\n",
    "\n",
    "# let's make a tensor that isn't contiguous and demonstrate the difference\n",
    "tensor_2d_non_contiguous = tensor_2d.t()\n",
    "print(tensor_2d_non_contiguous.is_contiguous())\n",
    "print(tensor_2d_non_contiguous.shape)\n",
    "try:\n",
    "    print(tensor_2d_non_contiguous.view(1, 4)) # tensor([[1, 2, 3, 4]])\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "# 2) matrix multiplication is key\n",
    "\n",
    "tensor_2d.matmul(tensor_2d.t()) \n",
    "print(tensor_2d.shape, tensor_2d.t().shape, tensor_2d.matmul(tensor_2d.t()).shape)\n",
    "\n",
    "# more compactly we can use @)\n",
    "\n",
    "print(tensor_2d @ tensor_2d.t())\n",
    "\n",
    "# 3) broadcasting is a powerful feature\n",
    "\n",
    "tensor_2d + 1\n",
    "\n",
    "# 4) indexing and slicing\n",
    "\n",
    "print(tensor_2d[0, 1])\n",
    "print(tensor_2d[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19. 22.]\n",
      " [43. 50.]]\n",
      "Number of columns in A must match number of rows in B\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# creating matmul from scratch in numpy\n",
    "\n",
    "def naive_matmul(A, B):\n",
    "    m, n = A.shape\n",
    "    p, q = B.shape\n",
    "\n",
    "    if n != p: raise ValueError(\"Number of columns in A must match number of rows in B\")\n",
    "\n",
    "    C = np.zeros((m, q))\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(q):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j] # pegs column of A to row of B and sums the products\n",
    "\n",
    "    return C\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(naive_matmul(A, B))\n",
    "\n",
    "assert np.allclose(naive_matmul(A, B), np.matmul(A, B))\n",
    "assert np.allclose(naive_matmul(A, B), A @ B)\n",
    "\n",
    "# 1) test identity matrix\n",
    "\n",
    "I = np.eye(3)\n",
    "assert np.allclose(naive_matmul(I, I), I)\n",
    "\n",
    "# 2) test dimension mismatch\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8], [9, 10]])\n",
    "\n",
    "try:\n",
    "    print(naive_matmul(A, B))\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# 3) test scalar multiplication\n",
    "\n",
    "I = np.eye(A.shape[0])\n",
    "assert np.allclose(naive_matmul(A, I), A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models as Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of z: tensor([3.6300])\n",
      "loss: tensor(3.6562)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# define the inputs\n",
    "y = torch.tensor([0.0]) # ground truth\n",
    "x1 = torch.tensor([3.3]) # input\n",
    "w1 = torch.tensor([1.1]) # weight\n",
    "b = torch.tensor([0.0]) # bias unit\n",
    "\n",
    "# forward pass\n",
    "z = x1 * w1 + b # linear layer\n",
    "print(\"output of z:\", z)\n",
    "a = torch.sigmoid(z) # activation\n",
    "loss = F.binary_cross_entropy(a, y) # loss function\n",
    "print(\"loss:\", loss)\n",
    "\n",
    "# torch goes ahead and builds computational graph in the background\n",
    "# it does this when a terminal node has requires_grad=True\n",
    "# This is crucial to the backpropagation algorithm, which is used \n",
    "# to update the weights and biases of the model during learning.\n",
    "\n",
    "# backpropagation is basically the chain rule applied to the graph to\n",
    "# compute the gradients of the loss with respect to the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of z: tensor([3.6300], grad_fn=<AddBackward0>)\n",
      "loss: tensor(3.6562, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "gradient of loss with respect to w1: (tensor([3.2148]),)\n",
      "gradient of loss with respect to b: (tensor([0.9742]),)\n",
      "weights after backpropagation: tensor([3.2148]) tensor([0.9742])\n"
     ]
    }
   ],
   "source": [
    "# let's run it back with backpropagation using autograd\n",
    "\n",
    "from torch.autograd import grad\n",
    "\n",
    "# inputs again\n",
    "y = torch.tensor([0.0]) # ground truth\n",
    "x1 = torch.tensor([3.3]) # input\n",
    "w1 = torch.tensor([1.1], requires_grad=True) # weight\n",
    "b = torch.tensor([0.0], requires_grad=True) # bias unit\n",
    "\n",
    "# forward pass\n",
    "z = x1 * w1 + b # linear layer\n",
    "print(\"output of z:\", z)\n",
    "a = torch.sigmoid(z) # activation\n",
    "loss = F.binary_cross_entropy(a, y) # loss function\n",
    "print(\"loss:\", loss)\n",
    "\n",
    "# backpropagation\n",
    "grad_loss_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_loss_b = grad(loss, b, retain_graph=True)\n",
    "\n",
    "print(\"gradient of loss with respect to w1:\", grad_loss_w1)\n",
    "print(\"gradient of loss with respect to b:\", grad_loss_b)\n",
    "\n",
    "# alternatively, we can use the backward method\n",
    "# that also computes the gradients of the loss with respect\n",
    "# to the weights and biases. this is the preferred method.\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"weights after backpropagation:\", w1.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bringing it all together: Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=10, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "number of parameters in the model: 1263\n",
      "Parameter containing:\n",
      "tensor([[-1.1927e-01, -1.2398e-03,  3.4817e-02, -2.1938e-02, -8.4743e-02,\n",
      "         -1.3332e-01,  2.4077e-02,  5.5628e-02, -9.1602e-02, -6.8016e-02,\n",
      "          5.8990e-02,  2.2892e-02, -1.2519e-01,  7.5482e-02,  1.0687e-01,\n",
      "         -7.2587e-02,  2.8433e-02,  5.8810e-02,  2.8852e-03, -2.6434e-02,\n",
      "          1.0930e-01, -4.0211e-02,  1.1209e-01,  1.3121e-01, -1.3723e-01,\n",
      "          6.6699e-02,  8.1584e-02,  1.2639e-01, -9.7397e-03,  9.5536e-02,\n",
      "         -1.0552e-01, -8.1127e-02, -5.8165e-02, -3.7421e-02, -4.8189e-03,\n",
      "          4.9556e-02,  6.3948e-03,  1.5154e-02,  1.9822e-02, -7.1256e-02,\n",
      "          2.9408e-02,  5.8757e-02, -1.1909e-01,  7.4363e-02,  7.6591e-02,\n",
      "         -1.2040e-01,  2.1504e-02,  5.2423e-02,  1.1595e-01,  7.5924e-02],\n",
      "        [ 6.0410e-02, -1.2483e-01,  7.4719e-02,  1.3189e-01,  1.3972e-01,\n",
      "          8.4685e-02, -1.0888e-01,  4.8713e-02, -1.1684e-01,  2.9443e-02,\n",
      "         -2.1428e-02, -1.1308e-01, -1.1767e-01,  1.0214e-02,  1.1488e-01,\n",
      "         -6.3379e-02, -1.3412e-01, -7.9955e-02,  5.1071e-02, -2.6931e-02,\n",
      "         -1.2937e-01, -1.4504e-02, -5.2988e-02,  1.0848e-01, -6.0503e-02,\n",
      "         -6.6553e-02,  5.5620e-02, -9.2088e-02, -2.3051e-02, -9.4744e-03,\n",
      "          1.2941e-01,  1.5235e-02, -1.0132e-02, -3.9251e-02,  6.8636e-02,\n",
      "          2.6742e-02,  6.4034e-03, -1.1373e-01, -1.3523e-01,  1.2264e-02,\n",
      "         -1.2161e-01,  9.8965e-02,  1.1574e-02,  1.4911e-02,  9.7884e-02,\n",
      "          5.0600e-02,  7.2024e-02,  3.1014e-02, -1.2578e-01,  7.3192e-02],\n",
      "        [-1.3922e-01,  1.3293e-01, -7.8982e-02,  1.0112e-01,  4.7680e-03,\n",
      "          4.3859e-03, -4.7625e-02, -9.8045e-02, -8.2200e-02, -5.7927e-02,\n",
      "          5.1119e-02,  7.1102e-02,  8.3590e-02, -1.3048e-01, -3.4069e-02,\n",
      "          1.0274e-01, -8.3268e-02, -1.0060e-01,  9.0304e-02,  5.9954e-02,\n",
      "         -6.2336e-02, -7.1987e-02, -1.2684e-01,  1.5031e-02,  8.1903e-02,\n",
      "         -1.1410e-01,  1.3580e-02,  8.7840e-02,  1.0723e-01,  8.8313e-02,\n",
      "          7.1857e-02, -6.1702e-02,  3.2513e-02,  9.0208e-02, -7.1464e-02,\n",
      "          9.5093e-02,  5.8269e-04, -1.3960e-01, -1.1086e-01, -4.4324e-02,\n",
      "         -1.3135e-01,  1.2269e-01, -6.3245e-02, -2.1855e-02,  1.2877e-01,\n",
      "         -3.4238e-02,  9.9960e-02, -1.2232e-01, -1.9289e-02, -7.5962e-02],\n",
      "        [ 7.5963e-02,  2.5589e-02,  1.2301e-01, -1.3354e-01, -3.0394e-02,\n",
      "          3.0521e-02,  6.5877e-02,  5.0586e-02, -5.3754e-02,  1.1662e-01,\n",
      "         -4.7896e-02,  1.0672e-01, -1.3749e-01,  1.2360e-01,  1.3864e-01,\n",
      "         -1.0656e-01,  1.0502e-01,  3.6242e-02, -2.2650e-04, -7.2165e-02,\n",
      "          5.5266e-02,  1.3723e-01,  2.0742e-02,  1.3638e-01,  5.2077e-02,\n",
      "          6.2796e-02, -2.6768e-02,  5.9364e-02, -8.8121e-02,  1.9509e-02,\n",
      "         -2.7056e-02,  1.2247e-01,  8.5632e-02,  8.4048e-02, -5.5153e-02,\n",
      "         -3.3280e-02, -8.0861e-02,  7.6498e-02,  1.0804e-01, -8.8880e-02,\n",
      "         -1.2955e-02, -1.0761e-01, -1.3626e-01,  7.4265e-02,  1.1118e-01,\n",
      "          4.7563e-02,  6.8336e-02, -5.9947e-03, -1.1224e-01, -9.7670e-02],\n",
      "        [-1.3712e-01,  6.1556e-02, -1.1637e-01, -1.2533e-01,  1.0289e-01,\n",
      "          4.5103e-02,  8.2453e-03, -1.3215e-01, -9.6814e-02, -7.2100e-03,\n",
      "         -1.1806e-01, -5.1442e-02, -5.2488e-02, -7.8240e-02,  2.9011e-02,\n",
      "         -5.8376e-02, -9.4769e-02, -3.9429e-02,  3.2463e-02,  4.6035e-02,\n",
      "          1.0830e-01, -1.1679e-01,  1.2245e-01,  1.1900e-01, -2.9137e-02,\n",
      "          5.4176e-02, -9.2539e-02,  9.3983e-02, -3.3871e-02, -5.9995e-02,\n",
      "         -1.1363e-01, -9.3393e-02, -4.2940e-02,  6.4206e-02, -7.1700e-02,\n",
      "          6.3395e-02,  6.9312e-02, -1.3796e-01,  7.0749e-02,  3.3650e-02,\n",
      "          5.4078e-02, -1.3650e-01,  8.7247e-03,  7.5429e-02,  3.4035e-03,\n",
      "          9.7383e-02, -1.0466e-01,  3.4123e-02,  1.3188e-01, -6.1054e-02],\n",
      "        [ 3.8597e-02,  3.6966e-02, -8.7124e-02,  6.4107e-02,  6.9621e-02,\n",
      "         -3.9999e-02,  9.5615e-02, -1.1306e-02,  4.0589e-02,  9.1850e-02,\n",
      "          1.0002e-01, -5.8202e-02,  4.8007e-02,  1.5980e-02,  8.1578e-02,\n",
      "         -8.2307e-03,  4.3330e-02,  1.3892e-01,  9.0007e-02,  6.7240e-02,\n",
      "          1.2808e-01,  8.0016e-02, -6.7306e-02,  7.9215e-02,  1.3749e-01,\n",
      "          1.3630e-01, -6.8574e-02, -1.4049e-01, -3.5516e-02,  1.3504e-01,\n",
      "          5.3679e-02,  9.4601e-02,  5.9530e-02,  7.3485e-02,  1.3121e-01,\n",
      "         -3.0758e-02,  3.3597e-02,  8.7108e-02, -1.3404e-01,  2.7695e-02,\n",
      "          3.7257e-02, -9.9418e-02, -4.2096e-02,  3.5362e-02,  3.6102e-02,\n",
      "         -1.7062e-02, -7.4829e-02,  1.0856e-01, -2.1920e-02, -9.0747e-02],\n",
      "        [-3.6641e-02,  6.0380e-02, -1.8049e-02, -3.7476e-03,  4.0660e-02,\n",
      "          1.0930e-01,  3.4817e-02, -9.6779e-02,  6.9035e-02, -5.8181e-02,\n",
      "          6.2476e-02,  1.2487e-01,  8.2029e-02, -2.9560e-02,  4.4878e-02,\n",
      "         -7.6063e-02,  9.9784e-02, -5.5199e-02,  1.0524e-01, -2.6277e-02,\n",
      "          1.3284e-01, -1.3468e-01, -9.1916e-02,  1.3853e-01, -1.1197e-01,\n",
      "         -1.3862e-01, -8.5559e-02, -9.5306e-02,  1.1498e-01, -2.0777e-03,\n",
      "          1.1866e-01,  1.1682e-01, -1.4220e-03,  1.4526e-02,  9.8997e-02,\n",
      "          4.8000e-02, -8.2186e-02,  1.1978e-01, -5.9452e-02,  4.1183e-02,\n",
      "          1.7562e-02, -4.9438e-02,  1.2239e-01, -4.4845e-02,  5.6136e-02,\n",
      "          1.3857e-02, -1.2722e-01,  9.3034e-02, -9.3314e-03,  1.0481e-01],\n",
      "        [-2.1839e-02,  4.7930e-02, -1.2874e-01,  7.4516e-02,  1.7224e-02,\n",
      "         -8.3951e-02,  1.3310e-01,  6.1644e-03,  1.1346e-01,  6.6203e-02,\n",
      "          1.2944e-01, -1.1056e-01, -1.3302e-01, -4.6095e-02, -3.0582e-02,\n",
      "         -1.1827e-01,  4.7585e-02, -1.0018e-01, -1.2090e-01,  1.0197e-01,\n",
      "         -1.0074e-01,  1.7413e-02,  7.1545e-02, -8.5571e-02, -2.0406e-02,\n",
      "         -6.1851e-02, -1.1609e-02, -1.3653e-02,  1.3801e-01,  1.0359e-01,\n",
      "         -4.6830e-02, -1.0931e-01,  1.8829e-02, -2.1064e-02, -1.1198e-01,\n",
      "         -7.9670e-02,  1.0413e-02, -6.4966e-03,  7.0899e-02,  1.3526e-01,\n",
      "         -8.4319e-02, -1.2254e-01,  8.3105e-02,  6.7115e-02, -1.0378e-01,\n",
      "          4.7965e-02, -1.0131e-01,  9.8847e-02, -1.0079e-01,  1.2159e-01],\n",
      "        [-3.5846e-02, -6.9293e-02,  1.1058e-01,  1.1662e-02, -1.2540e-01,\n",
      "         -6.9948e-02, -1.3340e-01,  4.3510e-02, -7.5063e-02,  7.4808e-02,\n",
      "          4.4772e-02, -2.6293e-02,  3.2305e-02, -1.4819e-02, -1.0468e-01,\n",
      "         -1.6575e-02, -7.5024e-02,  9.9751e-02,  1.1909e-01, -1.5835e-02,\n",
      "         -2.9046e-03,  1.4065e-01,  1.4127e-01, -4.2296e-02, -9.0347e-02,\n",
      "          9.9445e-02,  1.1350e-02,  1.2474e-02, -1.1913e-01,  1.2486e-01,\n",
      "         -1.1031e-02, -4.3202e-02,  1.2897e-01,  8.0958e-03, -1.3950e-01,\n",
      "          1.1622e-01,  1.0570e-01,  1.3159e-01,  8.0955e-02,  9.6421e-02,\n",
      "         -4.4473e-02, -1.3577e-01, -8.1135e-02,  4.0554e-03, -4.3046e-02,\n",
      "         -1.3064e-01, -1.2363e-01, -1.4892e-02,  6.5689e-02, -1.0190e-01],\n",
      "        [-4.1915e-02, -2.3138e-03,  1.0111e-01, -5.0350e-02,  4.3220e-02,\n",
      "         -1.3245e-01,  7.3327e-02, -4.3122e-02, -1.1585e-01,  3.2693e-02,\n",
      "          2.6050e-02, -1.7882e-04, -2.1242e-05, -1.2704e-01, -1.3574e-01,\n",
      "          5.8901e-02,  1.1131e-01,  7.4347e-02, -4.5294e-03,  7.5240e-02,\n",
      "          4.6139e-02, -7.1013e-02, -1.3500e-01,  2.2298e-03, -1.4241e-04,\n",
      "          3.6038e-02, -7.3650e-02,  8.8466e-02, -5.5126e-02, -5.7671e-02,\n",
      "          2.8624e-02, -7.3318e-02, -2.3438e-02,  4.7362e-02,  3.3074e-02,\n",
      "         -8.6594e-02, -7.3796e-02,  3.0647e-02, -9.3044e-02,  4.9882e-02,\n",
      "          3.9497e-04,  1.0047e-01, -1.2539e-01, -1.1607e-01,  4.0062e-02,\n",
      "         -5.2036e-02, -8.3909e-02, -1.1207e-01, -1.0265e-01,  1.2374e-01],\n",
      "        [ 1.2729e-01, -1.4004e-01, -1.2043e-01, -3.2477e-03,  1.0478e-03,\n",
      "         -1.1631e-01, -8.7057e-02,  1.7713e-02, -1.0458e-01, -5.0803e-02,\n",
      "          5.7025e-02,  1.2226e-01,  1.8654e-02,  1.0604e-01,  1.2621e-01,\n",
      "          2.6585e-02,  1.2398e-01,  1.1030e-01,  6.3690e-02, -6.0935e-02,\n",
      "         -4.8706e-02, -1.2100e-01,  7.6181e-02,  7.2174e-02,  1.2168e-01,\n",
      "          5.5817e-02,  3.4107e-03,  6.3327e-02,  5.7046e-02,  4.9140e-02,\n",
      "          4.7942e-03,  9.5283e-02,  1.0757e-01, -8.8875e-03, -5.2038e-02,\n",
      "         -1.2503e-01, -9.9400e-02,  6.4489e-03,  7.5537e-02,  2.0737e-02,\n",
      "          1.3407e-01, -4.9689e-02, -6.5452e-02, -2.2969e-02, -1.1206e-01,\n",
      "          1.2615e-02, -1.2567e-01, -3.9577e-02,  7.6173e-02, -1.1700e-01],\n",
      "        [-2.6408e-02,  5.1558e-02,  4.7013e-02,  6.1361e-02, -7.2455e-02,\n",
      "         -1.2361e-01,  9.5934e-02,  5.0148e-02, -1.0609e-01, -1.3104e-01,\n",
      "          2.8604e-02, -1.1683e-01, -1.1134e-01, -1.3079e-01, -1.1261e-01,\n",
      "         -8.3088e-02,  4.3108e-02,  9.1692e-02, -1.0789e-01, -1.1545e-01,\n",
      "         -8.2101e-02, -5.2979e-02,  1.0227e-01, -7.8691e-02, -2.4527e-02,\n",
      "          1.2830e-01,  4.4213e-02, -1.2063e-02, -2.0316e-02, -8.4673e-02,\n",
      "         -1.2049e-01,  1.2808e-01, -1.1312e-01,  6.8186e-02,  3.5605e-02,\n",
      "          9.3432e-02,  3.2349e-02, -1.0652e-01, -8.9473e-02,  1.6334e-02,\n",
      "         -1.0579e-01,  1.2558e-01,  5.3474e-03, -1.0538e-01, -1.4120e-01,\n",
      "          1.0503e-01,  1.3163e-01,  1.1413e-01, -3.6687e-02, -9.7511e-02],\n",
      "        [-7.2339e-02, -2.4705e-02, -3.7054e-02, -1.1964e-01,  2.9090e-02,\n",
      "         -2.3324e-02, -2.4069e-02,  9.6578e-02,  1.3689e-01,  1.2151e-01,\n",
      "         -2.5432e-02,  5.8525e-02, -1.2490e-01, -4.2752e-02,  1.4018e-01,\n",
      "         -8.2256e-02, -9.0774e-02, -1.8826e-02,  1.2164e-01,  2.9017e-03,\n",
      "         -1.4081e-01,  3.6782e-03,  1.0745e-01, -3.8290e-04, -1.3065e-01,\n",
      "          3.2325e-02, -1.2193e-01, -1.2768e-01, -3.6906e-02,  1.3397e-01,\n",
      "          2.1036e-02, -8.3389e-02,  6.6088e-02,  8.6495e-02,  1.2555e-01,\n",
      "          1.3790e-01,  7.4500e-02, -1.3071e-01, -8.7457e-03, -3.7801e-02,\n",
      "         -8.0114e-02,  1.3874e-01,  9.3650e-02, -6.6739e-02, -5.0971e-02,\n",
      "         -6.4423e-02,  1.1909e-01, -9.3864e-02, -8.7753e-03,  7.3506e-02],\n",
      "        [ 1.0571e-01,  1.6626e-02,  4.0600e-02,  9.4365e-02, -1.1924e-02,\n",
      "         -8.9775e-02, -1.4070e-01,  1.4134e-01,  2.3292e-02,  8.1995e-02,\n",
      "          5.3100e-02,  1.7772e-02,  6.4916e-02, -5.9825e-02, -1.2248e-01,\n",
      "         -5.0018e-02,  1.3679e-01,  1.2943e-01,  8.6537e-02, -4.3946e-03,\n",
      "          3.4615e-02, -1.1293e-01,  1.1718e-01,  1.1116e-01, -5.9754e-02,\n",
      "         -4.1496e-02, -3.3054e-02,  2.4611e-02, -4.4818e-02,  1.2089e-01,\n",
      "          5.6471e-02,  5.5339e-02, -5.3075e-02,  1.1033e-01, -6.9895e-02,\n",
      "          1.4291e-02,  1.3136e-01,  4.3879e-02, -6.3688e-02, -1.0311e-01,\n",
      "         -9.9687e-02, -1.9118e-02, -4.8346e-02,  1.1080e-01,  1.2248e-01,\n",
      "         -2.1979e-02, -1.0978e-01, -7.9915e-02,  7.1960e-02, -1.6192e-03],\n",
      "        [-1.2575e-01,  7.9762e-02,  2.3106e-02,  1.3543e-01,  8.5493e-02,\n",
      "          8.0211e-02,  3.0974e-02, -4.2350e-02, -1.0143e-01, -1.2590e-01,\n",
      "          1.0827e-01,  1.2253e-01, -9.9755e-02,  1.0307e-01, -8.8212e-02,\n",
      "          1.1564e-02, -1.0431e-01,  6.5038e-02,  8.6690e-02,  1.1193e-01,\n",
      "          1.2240e-01,  1.0843e-01,  1.0867e-01,  3.2186e-02, -1.2138e-01,\n",
      "         -7.6973e-02, -3.2775e-02, -6.2083e-02, -7.0593e-02,  3.9040e-02,\n",
      "          4.6331e-02, -1.2786e-01, -1.3040e-01, -1.0707e-01, -1.3875e-01,\n",
      "         -1.3568e-01, -1.0303e-01,  6.4614e-02, -7.4591e-02, -1.2303e-01,\n",
      "         -2.6216e-02,  1.3358e-02,  1.3079e-01, -1.3664e-01, -7.6730e-02,\n",
      "         -1.0475e-01, -1.3617e-01, -1.4004e-01,  1.9652e-02,  9.7094e-03],\n",
      "        [ 7.2580e-02,  8.5131e-02, -3.4560e-02, -1.1048e-01,  8.0068e-02,\n",
      "          7.1233e-02,  7.1024e-02, -1.1125e-01, -1.0758e-02, -4.0556e-02,\n",
      "         -6.8598e-04, -8.2545e-02, -1.2535e-01,  5.4846e-02,  2.2117e-02,\n",
      "         -4.4493e-02,  4.8064e-02, -2.5966e-02, -7.0933e-02, -7.5508e-03,\n",
      "         -1.1921e-01, -7.1770e-03,  1.0568e-01,  3.3668e-02, -7.5183e-02,\n",
      "         -2.5986e-02, -9.1210e-02,  1.3054e-01, -1.3808e-01, -9.5952e-03,\n",
      "          6.9521e-02,  1.8457e-02, -1.0546e-02, -1.0764e-01, -1.1876e-01,\n",
      "          1.2371e-01, -3.9606e-02, -4.7266e-02,  5.0267e-02,  7.7294e-02,\n",
      "         -7.5250e-02, -6.3889e-02, -1.2287e-01,  3.9971e-02, -1.0488e-01,\n",
      "          6.0392e-02,  6.5081e-02, -8.2570e-02,  1.0979e-01, -3.6948e-02],\n",
      "        [ 3.7474e-02,  6.4455e-02,  1.2640e-01, -8.9773e-02,  9.4163e-02,\n",
      "         -2.0445e-02, -1.8012e-02,  1.3255e-01,  3.7234e-02,  9.8827e-02,\n",
      "         -1.9497e-02, -1.0290e-01,  6.2178e-02, -1.0009e-02,  9.3385e-03,\n",
      "          1.1439e-01, -6.9519e-02,  1.0463e-01,  4.4546e-02,  6.6823e-02,\n",
      "         -7.0040e-02, -4.1250e-02, -2.5837e-02, -1.0699e-01,  2.7521e-02,\n",
      "         -1.3814e-01, -4.8482e-02,  1.4085e-01, -4.1228e-03,  1.0113e-01,\n",
      "         -1.3413e-01,  3.2772e-02,  8.6841e-02,  1.2099e-01,  2.6060e-02,\n",
      "          1.0494e-01, -6.8953e-02,  3.3014e-02, -1.6005e-02,  6.0717e-02,\n",
      "          1.1644e-01, -9.1537e-02, -1.4110e-01, -1.1513e-01,  1.2361e-01,\n",
      "         -1.3466e-01, -3.8642e-02, -1.3066e-01, -1.3061e-02,  1.0615e-01],\n",
      "        [ 6.5233e-02,  8.8672e-02,  1.2493e-01, -9.4818e-02, -1.3041e-01,\n",
      "          8.1365e-02,  6.7164e-02,  1.2552e-02, -1.2257e-01,  3.7514e-02,\n",
      "          1.0227e-01,  1.3817e-01,  9.2141e-02,  1.2163e-01, -9.4822e-02,\n",
      "         -8.8719e-02, -9.0084e-02, -1.3043e-02, -9.7978e-02,  3.8067e-02,\n",
      "          1.2912e-01, -2.1463e-02,  1.0742e-01,  1.0752e-01,  1.1068e-01,\n",
      "          1.3985e-01, -8.5564e-02, -1.1822e-01, -5.4489e-02, -3.4717e-02,\n",
      "          3.4788e-02,  7.7424e-02,  9.8026e-02,  7.9871e-02, -1.0298e-01,\n",
      "          1.3526e-01,  1.3249e-01, -2.6458e-02,  6.2409e-02,  1.1612e-01,\n",
      "          6.3477e-02,  1.1811e-04, -1.3558e-01,  6.5104e-02, -1.2222e-01,\n",
      "          8.0408e-02, -2.0896e-02,  2.9975e-02, -7.2717e-02, -4.0357e-02],\n",
      "        [-7.7271e-02, -1.0922e-01,  1.7238e-02, -1.1125e-01,  1.1393e-01,\n",
      "          2.0437e-02,  3.5103e-02,  7.5427e-02,  4.6082e-02,  1.0998e-01,\n",
      "          3.1147e-02,  5.1474e-02,  1.0629e-01, -8.5251e-02, -1.3227e-01,\n",
      "         -1.3121e-01, -8.8210e-02,  6.9897e-02,  2.2945e-02,  5.3739e-02,\n",
      "          9.2583e-02,  9.4970e-03, -1.0180e-01, -1.1887e-01, -8.2815e-02,\n",
      "         -1.1729e-01, -5.3258e-02,  5.4871e-02,  7.5947e-02,  5.1698e-02,\n",
      "          2.5319e-02, -9.3433e-02, -3.2104e-02,  5.1685e-02,  1.3705e-01,\n",
      "          6.3309e-02, -9.0994e-02, -9.2899e-02, -1.2703e-01,  9.7692e-03,\n",
      "          6.5199e-03, -1.0082e-01, -1.0613e-01, -9.8229e-02, -4.3440e-02,\n",
      "         -7.6099e-02, -1.7449e-02, -1.0215e-01, -1.1472e-01,  9.8303e-02],\n",
      "        [ 1.1133e-01,  1.2713e-01, -4.3402e-03, -1.6296e-02, -5.9229e-02,\n",
      "         -1.1507e-03,  1.1302e-02, -7.3484e-02, -5.8150e-02, -9.7656e-02,\n",
      "         -7.0298e-02, -6.1460e-02, -4.6224e-02,  9.4091e-02, -3.8518e-02,\n",
      "          2.0837e-02,  1.0504e-01, -1.2771e-01,  7.2624e-02, -3.0624e-02,\n",
      "          8.5579e-02, -1.2729e-01,  5.3040e-02, -8.7750e-02, -1.6529e-02,\n",
      "          1.0937e-01,  8.4148e-02,  2.1644e-02,  2.7340e-02,  3.1626e-02,\n",
      "         -1.0521e-01, -3.9322e-02,  3.0916e-02,  1.1367e-01,  1.2048e-01,\n",
      "          7.4787e-02,  1.6789e-02, -1.2666e-01, -1.2211e-01,  1.6021e-02,\n",
      "          1.4079e-01, -1.9915e-02, -3.4000e-02,  1.0844e-01, -5.5550e-02,\n",
      "         -4.3952e-02,  3.6882e-02,  1.0146e-01,  8.2892e-02,  6.5378e-02]],\n",
      "       requires_grad=True)\n",
      "torch.Size([20, 50])\n"
     ]
    }
   ],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            \n",
    "            # first hidden layer\n",
    "            torch.nn.Linear(input_dim, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # second hidden layer\n",
    "            torch.nn.Linear(20, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # logits layer\n",
    "            torch.nn.Linear(10, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x) # returns logits\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "model = MLP(input_dim=50, output_dim=3)\n",
    "print(model)\n",
    "\n",
    "# ok.. so a few interesting things here. how does an MLP work?\n",
    "# well in this scenario it routes information through a hierarchical set\n",
    "# of \"questions\" such that the activations represent the \"answers\" to those questions.\n",
    "\n",
    "# how does a MLP differ to say a CNN? MLPs process every input feature with \n",
    "# full connectivity, whereas CNNs use local spatial maps, often in parallel, \n",
    "# to process the input.\n",
    "\n",
    "# what about RNNs? RNNs can be thought of as special MLPs that process sequential data recursively.\n",
    "\n",
    "print(\"number of parameters in the model:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# oh, it's a little cutie. \n",
    "\n",
    "print(model.layers[0].weight)\n",
    "\n",
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0340, -0.2001,  0.3014]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0340, -0.2001,  0.3014]])\n",
      "tensor([[0.3228, 0.2554, 0.4218]])\n",
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# inference baby\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "x = torch.randn(1, 50)\n",
    "\n",
    "print(model(x))\n",
    "\n",
    "# notice the AddmmBackward specifies how the output is computed.\n",
    "# we don't actually need this extra computation and memory for inference\n",
    "# so we can use the torch.no_grad() context manager to disable gradient computation.\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(model(x))\n",
    "\n",
    "# what about the probs? these are only logits.\n",
    "# logits are effectively the raw output of the last layer\n",
    "# they represent the unnormalised scores for each class\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(torch.softmax(model(x), dim=1))\n",
    "\n",
    "# now we have class membership probabilities\n",
    "# we can use the argmax method to get the predicted class\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(torch.argmax(torch.softmax(model(x), dim=1), dim=1))\n",
    "\n",
    "# this is the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
